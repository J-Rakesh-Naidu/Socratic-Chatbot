{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-RecnkGjWFF"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "\n",
        "\n",
        "# Initialize components\n",
        "DATA_PATH = \"/content\"\n",
        "DB_FAISS_PATH = \"/content\"\n",
        "HUGGINGFACE_REPO_ID = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "def initialize_components(user_input=None, conversation_history=None): # Added parameters for user_input and conversation_history\n",
        "    # Load or create vector store\n",
        "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "    if os.path.exists(os.path.join(DB_FAISS_PATH, \"index.faiss\")):\n",
        "        db = FAISS.load_local(DB_FAISS_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "    else:\n",
        "        # Load and process documents if vector store doesn't exist\n",
        "        loader = DirectoryLoader(DATA_PATH, glob='*.pdf', loader_cls=PyPDFLoader)\n",
        "        documents = loader.load()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "        text_chunks = text_splitter.split_documents(documents)\n",
        "        db = FAISS.from_documents(text_chunks, embedding_model)\n",
        "        db.save_local(DB_FAISS_PATH)\n",
        "\n",
        "    # Initialize LLM\n",
        "    login(token=HF_TOKEN)\n",
        "    llm = HuggingFaceEndpoint(\n",
        "        repo_id=HUGGINGFACE_REPO_ID,\n",
        "        task=\"text-generation\",\n",
        "        temperature=0.5,\n",
        "        model_kwargs={\"token\": HF_TOKEN, \"max_length\": 512}\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "       memory_key=\"chat_history\",\n",
        "       return_messages=True,\n",
        "       # output_key=response['result']  # Remove or adjust this line based on your intended usage\n",
        "       input_key='question', # Setting 'input_key' to store user's questions\n",
        "       output_key='answer'\n",
        "   )\n",
        "    # Socratic-style prompt template\n",
        "    custom_prompt = PromptTemplate(\n",
        "        template=\"\"\"You are a Socratic AI that helps users learn by asking thoughtful questions.\n",
        "        Use the provided context to guide your questioning, but never give direct answers.\n",
        "\n",
        "        Context: {context}\n",
        "\n",
        "        Current conversation: {chat_history}\n",
        "\n",
        "        User's question: {question}\n",
        "\n",
        "        Instructions:\n",
        "        1. Ask a single, thought-provoking question related to the user's query.\n",
        "        2. The question should encourage reflection and deeper understanding.\n",
        "        3. Avoid giving direct answers or hints.\n",
        "        4. Base your next question on the user's response to the previous one, maintaining a Socratic dialogue.\n",
        "        Example:\n",
        "        User: What are factorials?\n",
        "        AI: Hmm, interesting! Can you think of a way to calculate how many different ways you can arrange 3 objects?\n",
        "\n",
        "        Remember, always encourage the user to explore ideas and avoid giving direct answers.\n",
        "        Socratic response:\"\"\",\n",
        "        input_variables=[\"context\", \"chat_history\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Create the conversational chain\n",
        "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=db.as_retriever(search_kwargs={'k': 3}),\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\"prompt\": custom_prompt},\n",
        "        return_source_documents=True,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    return qa_chain\n",
        "\n",
        "conversation_history = []\n",
        "print(\"Socratic Chatbot: Hello! Let's explore a topic together. What would you like to know?\\n\")\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\"]:\n",
        "        print(\"Socratic Chatbot: Great conversation! See you next time. ðŸ‘‹\")\n",
        "        break\n",
        "\n",
        "    # Get Socratic-style response\n",
        "    qa_chain = initialize_components(user_input, conversation_history) # Calling with user_input and conversation_history\n",
        "    response = qa_chain({\"question\": user_input}) # Calling the run method with user input to get the result.\n",
        "    print(f\"Socratic Chatbot: {response['answer']}\") # Accessing the response from the 'result' key\n",
        "\n",
        "    # Update conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    conversation_history.append({\"role\": \"assistant\", \"content\": response['answer']}) # Accessing response from 'result' key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEj0CYM0sfAP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
